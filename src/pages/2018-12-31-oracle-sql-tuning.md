---
title: Oracle 数据库 SQL 性能调优实践笔记
subtitle: 亿级数据量下 Oracle 数据库 SQL 性能调优初级解决方案。
date: 2018-12-31 19:54:38 +8
tags:
  - Oracle
  - SQL
  - Database
---

> ETL： Extract-Transform-Load，一个数据清洗 ✨ 的过程。

目前项目正在构建一个 ETL 系统，做数据的清洗 ✨。现在已经向客户请求了“真实数据”，正在做最后的线上环境测试。而在线上测试的过程中，出现了几个性能上有问题的 SQL，这篇文章对解决这些性能问题的过程和方法做一个笔记。不过在进入正文之前，我想说一下出现这些问题的原因。

关于出了问题的几条 SQL 涉及的表，在得到“真实数据”之前，我们其实是不太清楚其数据规模的，而在之前的测试中，也只是最多几百条数据的情况下做的，还体现不出来问题，但是当数据量达到千万级和亿级的时候，性能差距就很明显了。

其根本原因还是在设计阶段，没有明确和客户讨论过表的数据规模，开发阶段也没有带入对性能问题的考虑，最终导致了本地少量数据测试完全没问题，到了线上大规模数据下测试一下就暴露性能问题的结果。

---

### 案例和解决方案

我们先来看一遍所有的案例、解决方案，和大概的思路，至于具体的思路和方法在最后详细说明。

本次实践的难度为初级，实际用到的解决方法从易到难，有以下几种：

- 添加索引，针对表读取大于写入的场景；
- 优化有性能问题的关键字；
- 修改业务逻辑。

当我们开始调整一条 SQL 时，第一步一般是查看优化器的执行计划，大致定位出问题的原因。通常情况下，出现性能问题的原因都是由于某一段 SQL 走了全表检索，遍历了一张大表。对于这种情况，如果没有相应的索引的话，通过建立索引表可以得到明显改善。不过如果是存在索引但是没有走索引的情况下，会需要使用 `hint` 来强制优化器使用指定的索引来达到效果。

某些关键字也会影响优化器的策略，我们遇到一个案例，一段 SQL 不合时宜的使用了 `in` 关键字造成性能问题，添加索引没有效果，最终将其转化为 `inner join` 配合索引有了明显的效果。

修改业务逻辑的方法应用场景比较少，但有时会有奇效。例如在我们遇到的一个问题中，正常的业务逻辑下需要更新大量的数据，结合数据和业务逻辑分析，我们发现可以通过修改业务逻辑，在保持功能性的前提下减少大部分的更新量。这个案例在后面还会进一步说明。

除此之外还会稍微记录一下下面这些解决方案，这些方案有些是研讨中还没有运用到实际问题上去的，不过都是些确实有效果的方法：

- 使用 partition 表分区提高效率；
- 减少 log 提高写入效率；
- 分割复杂 SQL 为多个简单 SQL，用中间表减少内存负担；
- 用 `merge` 代替 `update` 做数据更新提高效率；
- 压缩表，利用空闲 CPU 减少 IO 时间。

下面来看看具体的案例。

#### 添加索引

添加索引可以大幅提高查询速度，但是代价是插入和更新语句会相应变慢。索引提高查询速度的原理是“空间换速度”，当创建一个索引的时候，数据库会提取出所有数据的相应字段创建一张索引表，这样如果一个查询语句使用了该索引，那么这条语句只会对索引表进行检索，以此来提高速度。

举例说明：

如果一张表有 150 个字段，一个查询 SQL 的 `where` 条件仅包含了其中 5 个字段，此时如果没有索引的话，这条查询 SQL 会对有 150 个字段的全表进行检索。

这个情况下如果对查询 SQL 所包含的 5 个字段创建一个联合索引，那么最终的结果，这个查询 SQL 只会对有 5 个字段的索引表进行检索。相对于全表检索，肯定是索引表检索的效率更高。

创建索引表有负面影响，其一是会占用更多空间。围绕上面这个例子 🌰 来说，创建索引表会提取所有数据的相应的 5 个字段单独储存来提高速度。

第二个影响是会让插入和更新变慢。当没有索引表的情况下，一个插入或更新语句只需要对主表操作即可，但是如果创建了索引表，插入或更新语句在操作完主表之后，还要更新相应的索引表。

总结：索引适合一张表读取多于写入的情况。索引仅对查询 SQL 有效果，包括子查询。索引越多，对 DML 语句影响越大，合适对策略是仅创建够用的索引。

具体代码不方便展示，下面仅展示几个修改前后执行效率的例子。当然最后会用测试数据做一个重现。

**例子 🌰1**

| 输入表     |      数据量 |
| ---------- | ----------: |
| 顾客表     | 138,066,840 |
| 顾客信息表 | 145,586,869 |
| 业务表     |      16,220 |

输出数据量为：7,442。

| &nbsp;     | 运行时间 |
| ---------- | -------- |
| 创建索引前 | 2:02:27  |
| 创建索引后 | 0:01:17  |

这个例子中，虽然最终输出结果仅为 7 千行，但由于 `顾客表` 和 `顾客信息表` 都是拥有百来个字段的大表，全表检索非常吃力。优化的方法是对所有查询条件的字段创建了索引，效果很明显。

**例子 🌰2**

| 输入表 |      数据量 |
| ------ | ----------: |
| 顾客表 | 138,066,840 |
| 业务表 |  43,850,757 |
| 种类表 |          16 |

输出数据量为：43,599,931。

| &nbsp;     | 运行时间 |
| ---------- | -------- |
| 创建索引前 | 3:10:17  |
| 创建索引后 | 1:12:23  |

这个例子由于四千三百万的输出数据量的限制，仅 IO 操作都会花费不少时间，添加索引后把查询部分的时间减少了不少，最终结果是 72 分钟，还算可以接受的结果。

#### 优化有性能问题的关键字

还是之前说到的那个案例。

**例子 🌰3**

| 输入表       |      数据量 |
| ------------ | ----------: |
| 顾客信息表 1 | 145,586,869 |
| 顾客信息表 2 | 294,225,728 |
| 顾客信息表 3 | 261,309,380 |
| 顾客信息表 4 | 140,379,648 |

输出数据量为：149。但是对上面四张表做了不同程度的更新。

| &nbsp; | 运行时间 |
| ------ | -------- |
| 修改前 | 1:58:48  |
| 修改后 | 0:00:03  |

这个案例中，SQL 包含一连串的 `with as` 语句，其中一块语句的查询条件里面存在两个 `in` 关键字，但是 `in` 语句里面都是亿级的大表，造成严重的性能问题，最终通过将其改写为两个 `inner join` 得到改善。

#### 修改业务逻辑

修改业务逻辑的场景一般没有普适性，需要结合数据和业务逻辑单独分析。

**例子 🌰4**

| 输入表     |      数据量 |
| ---------- | ----------: |
| 顾客表     | 138,066,840 |
| 顾客信息表 | 145,586,869 |

输出数据量为：13,764。并且途中做了几次不同程度的更新。

| &nbsp; | 运行时间 |
| ------ | -------- |
| 修改前 | 5:05:53  |
| 修改后 | 0:01:00  |

这个例子中，在正常逻辑下，我们抽出一批数据插入到目标表中，将一个 Flag 更新为初始值 `00`，然后根据各种表关联的结果再将其中一部分数据更新为 `99`。但是实际数据测试中发现需要更新为 `99` 的数据量太大了，同时 `update` 本身又是一个 cost 很高的操作，所以整段 SQL 的执行效率奇慢无比。分析数据和业务逻辑之后，我们将 Flag 初始化更新为 `99`，然后将**不符合条件的数据**更新为 `00`，以此来减少了大量更新操作，大幅度提高了执行效率。

#### 一些其他的解决方案

上面是有具体案例的实践，下面还有一些试验过有效果的方案，但是由于一些原因没有运用到这次遇到的问题上去。都是一些有价值的方法，所以在这里也记录一下。

**使用 partition 表分区提高效率**

创建表分区虽然没有用来解决这次遇到的问题，但是实际上当初设计表的时候已经是做过分区的了。

表分区具体的作用是根据一定条件将一张大表分为几个不同的 `partition`，对外部来说这张表还是一个整体，但是优化器会自动判断，在符合条件的情况下访问对应的分区来提高效率。当然也可以在 SQL 语句中表名后面添加 `partition(partition_name)` 小句来指定访问哪个分区。

```sql
select count(1) from target_table partition(table_part01);
```

举一个具体的例子来说，假设顾客表中有两种类型的顾客，`企业` 顾客和 `个人` 顾客，这张表有 1 亿数据，如果我们按照顾客类型来给表进行分区，并且刚好每个分区有 5 千万数据的话，这时如果我们查询某个 `企业` 顾客的数据，查找范围就会从原本的 1 亿数据直接降到 5 千万数据，效率可谓直接提升一倍。

这个例子的 DDL 可能是下面这样的：

```sql
create table customer (
  id number,
  name varchar2(256),
  customer_type varchar2(20),
  -- other fields
) partition by list (customer_type) (
  partition customer_enterprise values ('enterprise'),
  partition customer_individual values ('individual')
);
```

> ⚠️ 表分区仅在企业版本可用。

官方文档建议的应该考虑创建表分区的场景：

- 表占空间大于 2G 以上；
- 表包含历史数据，典型的例子是按照月份整理的数据，仅更新当前月份的数据，而其他 11 个月的数据都是只读的；
- 表的内容必须分布到不同存储设备上。

参考资料：

- [2 Partitioning Concepts](https://docs.oracle.com/en/database/oracle/oracle-database/12.2/vldbg/partition-concepts.html#GUID-E849DE8A-547D-4A2E-9324-706CAF574754)

**减少 log 提高写入效率**

Oracle 中在执行 DML 语句时会产生 `UNDO` 和 `REDO` 日志来保护数据的完整性和安全性。`UNDO` 日志用来做事务的回滚，`REDO` 日志用来恢复事务。但是在大数据的情况下，大量的日志会很大程度地拉低执行效率。这时可以使用 `/*+ APPEND */` 来减少甚至不做 `UNDO` 日志，配合 `nologging` 来减少 `REDO` 日志，以此来提高执行效率。

使用 `/*+ APPEND */` 的 SQL 语句看起来是这样的。

```sql
insert /*+ APPEND */ into target_table (
  target_values,
  -- ...
);
```

使用 `nologging` 有两种方式，一种是从表定义上设定。

```sql
alter table target_table nologging;
```

或者写在 DML 语句中。

```sql
insert /*+ APPEND */ into target_table nologging (
  target_values,
  -- ...
);
```

**分割复杂 SQL 为多个简单 SQL，用中间表减少内存负担**

我们遇到的一些问题中，有一些比较复杂的 SQL 用 `with as` 做了一堆子查询，然后再将这些结果 `union` 到一起成为一个临时表，然后拿这个临时表继续往后面 `with as` 出更多的临时表...以此类推的情况，单独拿出其中每一个 `as` 小句出来执行效率都是可以接受的，但是由于 `union` 等操作花费时间比较长，占用资源无法释放等于提升了 cost。

对于这种情况，我们将复杂 SQL 分割成几条不那么复杂的 SQL，比如上面说的 `union` 的问题，实际代码上将 3 个子查询结果 `union` 到了一起，修改后在中间加一个中间表，三串子查询也变成三个独立的 SQL 语句，分别向中间表中做 3 次插入操作，以此来避免 `union`。

当然这样也带来了一些其他的问题，比如写中间表的操作会带来很多额外的 IO 成本，如果后续处理数据不是很多的话，还会让原本执行效率高的的操作因为多了一个写中间表的 IO 成本而拉低执行效率。

一个真实的例子是，我们有一条 SQL 文要更新 1 万条左右的数据，但是中间访问了几张 1 亿多条数据的大表所以造成执行效率很慢，我们通过添加了中间表的方式，将一个重复多次的子查询结果固定到一张中间表，数据一共四千八百万，IO 操作花了大半个小时。对于这次调整的结果来说确实提升了执行效率，但是更新 1 万条数据的情况属于特殊情况，平时可能只更新少量的数据，却由于多了这么一个 IO 操作导致每次无论最终处理数据量，这大半个小时的时间都是避免不了的了。

**用 `merge` 代替 `update` 做数据更新提高效率**

遇到性能问题的 SQL 有相当一部分是更新操作，甚至有一个任务要更新 700 万数据的 1 个 Flag，因为写满 `UNDO` 空间死活跑不过去的。可想而知更新操作是非常消耗资源的。

Oracle 中有一个 `merge` 语句，原本是用来聚合 `insert` 和 `update` 操作的，但是在新版本中可以单独做更新或者插入了，从网上资料中我们了解到 `merge` 操作比单独的 `update` 操作执行效率更高，于是将有问题的几个 SQL 用 `merge` 改写试了试。

其结果是，一条更新 45 万数据的 `update` 需要跑 7 个小时的操作，使用 `merge` 只花了 50 分钟就完成了。目前这个方案正在研讨中，还没有最终决定，但是使用 `merge` 可以提升更新效率是毋庸置疑的。

```sql
merge into table_a a using table_b b
on (conditions)
when matched then
  update set a.fields = b.fields
when not matched then
  insert(field_names) values(field_values);
```

如果仅做更新操作，`WHEN NOT MATCHED THEN` 后面的语句可以不需要。

**压缩表，利用空闲 CPU 减少 IO 时间**

作为最后的手段，压缩表也是提升执行效率的大杀器。

压缩表的原理是，给相同数据创建字典表，每次有相同数据进来，数据库将不储存原始数据，而仅储存字典表的一个引用。好处是压缩了表空间，大幅减少了 IO 操作，不过缺点也很明显，会显著提高 CPU 使用率。

对我们项目来说，数据库使用独立服务器，CPU 经常是闲置的，所以压缩表没有太大负担。

```sql
alter table target_table compress;
```

// TODO reproduce
