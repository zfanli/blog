---
title: Oracle 数据库 SQL 性能调优实践笔记
subtitle: 亿级数据量下 Oracle 数据库 SQL 性能调优初级解决方案。
date: 2018-12-31 19:54:38 +8
tags:
  - Oracle
  - SQL
  - Database
  - BigDate
---

> ETL： Extract-Transform-Load，一个数据清洗 ✨ 的过程。

目前项目正在构建一个 ETL 系统，做数据清洗 ✨。到现在已经拿到“真实数据”在做最后的线上环境测试。

在测试的过程中，出现了几个性能上有问题的 SQL，这篇文章对解决这些性能问题的过程和方法做一个笔记。不过在进入正文之前，我想说一下问题出现的原因。

- 设计时不清楚表的数据规模，未考虑性能上的影响
- 开发时过于注重遵从设计，写法没有考虑性能优化
- 测试中数据量少，体现不出来问题

其根本原因还是在设计阶段的疏忽，设计 ETL 系统掌握每张表的数据规模也是非常重要的事情，有助于针对性的优化某些任务，优化时间分配。

---

### 写在前面

篇幅略长，主要是想对项目上最近解决的一些 SQL 性能问题做一个笔记。笔记包括问题出现的原因，解决的过程以及结果，最后动手做个再现和验证。下面是这篇文章的 road map。

- 概括部分
  - 罗列具体实践的调优方法和试验过有效的方法
  - 总结优化的大致思路
  - 这部分不涉及案例但会有少量代码示例
- 案例部分
  - 介绍几个挑选出来的典型案例和相关的解决方案
  - 这部分不会有代码示例
- 实践部分
  - 重现几个案例并且套用解决方案尝试效果
  - 步骤从搭建测试环境和准备测试数据开始
  - 这部分将有大部分代码示例

### 解决方案和思路

这是概括部分。

#### 解决方案

具体运用了的解决方法有以下几种：

- 添加索引（针对表读取大于写入的场景）
- 优化和改写相同逻辑（让其使用索引）
- 切割复杂语句到多个简单语句（主要优化内存占用）
- 使用 `merge` 代替 `update`
- 修改业务逻辑（效果好但没有普适性）

未运用到实际问题上，但是试验过有效的方案有下面这些：

- 使用 `partition` 表分区提高效率
- 使用 `/*+ APPEND */` 配合 `nologging` 减少日志提高写入效率
- 表压缩，利用空闲 CPU 减少 IO 时间

#### 优化思路

一般遇到问题时先尝试最简单的索引法。

**配合执行计划分析 SQL 定位问题**

查看执行计划最好的方式是打开 `autotrace`，执行下面语句之后，每条 SQL 将会返回实际的执行计划可供分析。前提是需要 DBA 权限。

```sql
set autotrace on;
```

但如果你和我一样没有 DBA 权限，那就只能看推测的实行计划了。在 SQL Developer 里选中一条 SQL 按 `F10` 可以查看精简版的执行计划，好处是方便快捷。

> 我使用的 Oracle SQL Developer 版本：3.2.20.10

除此之外还有一个可查看信息更多的解释计划语句，写在每条 SQL 前面，查看**推测的执行计划**。注意，重点是“推测”，由于没有 DBA 权限，这种解释计划仅是基于统计信息分析出来的“Guess”，并非实际采用的执行计划。同理上面的 `F10` 也是仅供参考的，一般保证表的统计信息分析正确的话还是挺准的。

```sql
explain plan for
  -- 接具体的语句
  select 1 from dual;

-- 使用下面的语句来查看执行计划
select plan_table_output from table(dbms_xplan.display())；
```

参考文档：

- [Tuning SQL\*Plus - Tracing Statements](https://docs.oracle.com/database/121/SQPUG/ch_eight.htm#SQPUG534)
- [Database SQL Language Reference - EXPLAIN PLAN](https://docs.oracle.com/database/121/SQLRF/statements_9010.htm#SQLRF01601)
- [Database PL/SQL Packages and Types Reference - DBMS_XPLAN](https://docs.oracle.com/database/121/ARPLS/d_xplan.htm#ARPLS378)

**添加索引并配合使用 `hint` 让不走索引的部分强制走索引看效果**

尝试给没有走索引的地方创建相应的索引。

```sql
create index index_name on table_name (fields...);
```

如果创建了相应索引，但因为优化器判断不去走索引的情况，可以用 `hint` 强制走索引看看效果。`index()` 的括号里面一般填两个参数，第一个为表名，第二个是索引名。

```sql
select /*+ index(target_table target_index) */
  fields...
from target_table;
```

> ⚠️ 创建索引的代价是会占更多的储存空间，并且每次插入数据的时候都会对索引进行维护，对写入速度产生影响。

一般来说，到这里很多问题都可以得到解决，尤其是读大表的场景，可能慢就慢在没有索引上。如果问题还没得到解决，我们再往后看。

参考文档：

- [Database SQL Language Reference - INDEX Hint](https://docs.oracle.com/database/121/SQLRF/sql_elements006.htm#BABEFDFC)
- [Database SQL Language Reference - USE_NL Hint](https://docs.oracle.com/database/121/SQLRF/sql_elements006.htm#BABDDFHC)

**对无法走索引的部分进行改写最终让其走索引**

某些语句的写法也会导致无法利用索引。在我们遇到的问题中就有一个，使用了 `in` 关键字导致无法走索引，最终通过改写 SQL 将其转化为 `inner join` 才得以使用索引，提高了执行效率。代码无法直接贴出来，做个重现的例子，大致如下。

```sql
-- 原始语句
select
  field_names...
from
  table_a a
where
  a.the_key in (select the_key from table_b);

-- 修改后语句
select
  field_names...
from
  table_a a
inner join
  table_b b
on
  a.the_key = b.the_key;
```

优化索引当然不能解决所有的问题，所以如果索引解决不了问题，就要结合更多信息来分析定位问题所在了。

**分割复杂 SQL 到多个简单 SQL，减少内存占有时间**

如果遇到内存问题，如 PAG 写满了，或者 UNDO 空间写爆了，考虑这个方案。

子查询嵌套过多会加剧内存资源的消耗，也会拉长内存占有时间。这时可以考虑分割复杂的 SQL 语句，加入几个中间表来缓和内存的负担。毕竟对于系统稳定性来说，增加的那点储存空间总是微不足道的。

**使用 `merge` 代替 `update` 缩短更新操作时间，减少内存占有时间**

`merge` 是一个可以同时进行插入和更新的聚合操作，而在最新版本中已经支持单独的更新和插入操作。经过我们的测试发现，用 `merge` 来进行更新操作执行效率比 `update` 要好，数据量大的情况下甚至 `merge` 的效率高出几倍。`merge` 示例如下。

```sql
merge into table_a a using table_b b
on (a.pk = b.pk)
when matched then     -- 当满足 on 里的条件执行更新语句，可省略
  update set a.flag = b.flag
when not matched then -- 当不满足 on 里的条件执行插入语句，可省略
  insert (a.pk, a.flag, a.other_fields)
  values (b.pk, b.flag, b.other_fields);
```

尝试用更有效率的写法改写 SQL，减少内存占有时间，也是优化内存出问题的一种方法。目前我们遇到带报错信息的问题还只有内存问题一个。

参考文档：

- [Database SQL Language Reference - MERGE](https://docs.oracle.com/database/121/SQLRF/statements_9017.htm#SQLRF01606)

如果上述方案都没有太大效果，那么可能你的瓶颈出现在 IO 操作或者是 `merge` 和 `update` 等非常昂贵又不可避免的操作上了，这时就可以尝试从业务的角度上优化。

优化业务逻辑风险大，且能否成功还是随缘的。不过是如果成功，效果就会很美好。

目前遇到一个案例，通过修改业务逻辑后，效率从原本的 2 个小时提升到了 3 秒。这个案例在之后会详细介绍，虽然方案没有普适性，但是可以作为参考。

一般优化业务逻辑可以做下面的考虑。

**数据更新的场景，可以结合数据分析，寻找业务逻辑的优化点**

更新是相对来说毕竟昂贵的操作，结合数据分析考虑是否能减少更新量将有助于提高效率。

举例来说，有一个业务要求抽取一批数据统一加上一个 Flag，设为 `00`，然后根据一堆条件筛选出其中一部分数据把 Flag 更新为 `99`。在这个业务上出现了性能问题，其原因是需要更新为 `99` 的数据太多了。

当知道性能问题真正的原因，事情就会简单很多。在这个栗子 🌰 中，由于需要更新的数据超过了处理数据的一半以上，于是，将其初始化为 `99`，然后将那些不符合条件的数据找出来更新为 `00` 可以有效的减少更新的数据量，提高效率。

这个栗子其实是一个具体的案例，之后还会提到。

**分析子查询，将重复的查询固化到一张工作表中，减少反复查询**

减少 IO 永远都是优化的首选方案，但实际上可能任务的每一步 IO 都是不可避免的。但是不同任务之间可能会有重复的逻辑，尝试分析类似的业务，找到反复查询的数据，将其临时固化到工作表，减少 IO，提高效率。

**针对多步骤的业务，提取所有筛选条件到第一步，缩小数据范围**

设计的时候有些地方由于逻辑的连贯性，会将筛选条件分布到多个步骤上，来让设计更可读，更好理解。

还是举例子来说，假设有一个业务分为两步，第一步根据处理条件筛选出 4800 万数据，放在一张中间表里，第二步根据这张中间表的数据进一步筛选出 1 万条数据，更新这些数据的 Flag，更新的条件是原本为 `0` 的更新为 `1`，原本为 `1` 的更新为 `0`。

这里的 IO 有两步，第一步读取 4800 万数据再写入 4800 万数据，第二步读取 4800 万数据，然后在其中找到 1 万数据用来更新。但是整体看下来这 4800 万数据其实还是存在优化余地的，因为这个业务的第二步隐藏了一个筛选条件，Flag 的值需要限定在 `0` 和 `1`。

这个条件如果在第一步加入进来，那么这 4800 万数据就能在一定程度得到减少，由此后续的 IO 操作都能减轻部分负担。

### 案例

这是案例部分。关于执行时间，可能会因为当时服务器的负载状态而产生一定的偏差，优化效果仅供参考。

#### 添加索引（针对表读取大于写入的场景）

**例子 🌰1**

| 输入表     |      数据量 |
| ---------- | ----------: |
| 顾客表     | 138,066,840 |
| 顾客信息表 | 145,586,869 |
| 业务表     |      16,220 |

输出数据量为：7,442。

| &nbsp;     | 运行时间 |
| ---------- | -------- |
| 创建索引前 | 2:02:27  |
| 创建索引后 | 0:01:17  |

这个例子中，虽然最终输出结果仅为 7 千行，但由于 `顾客表` 和 `顾客信息表` 都是拥有百来个字段的大表，全表检索非常吃力。优化的方法是对所有查询条件的字段创建了索引，效果很明显。

**例子 🌰2**

| 输入表 |      数据量 |
| ------ | ----------: |
| 顾客表 | 138,066,840 |
| 业务表 |  43,850,757 |
| 种类表 |          16 |

输出数据量为：43,599,931。

| &nbsp;     | 运行时间 |
| ---------- | -------- |
| 创建索引前 | 3:10:17  |
| 创建索引后 | 1:12:23  |

这个例子由于四千三百万的输出数据量的限制，仅 IO 操作都会花费不少时间，添加索引后把查询部分的时间减少了不少，最终结果是 72 分钟，还算可以接受的结果。

#### 优化和改写相同逻辑（让其使用索引）

**例子 🌰3**

| 输入表       |      数据量 |
| ------------ | ----------: |
| 顾客信息表 1 | 145,586,869 |
| 顾客信息表 2 | 294,225,728 |
| 顾客信息表 3 | 261,309,380 |
| 顾客信息表 4 | 140,379,648 |

输出数据量为：149。但是对上面四张表做了不同程度的更新。

| &nbsp; | 运行时间 |
| ------ | -------- |
| 修改前 | 1:58:48  |
| 修改后 | 0:00:03  |

这个案例中，SQL 包含一连串的 `with as` 语句，其中一块语句的查询条件里面存在两个 `in` 关键字，但是 `in` 语句里面都是亿级的大表，造成严重的性能问题，最终通过将其改写为两个 `inner join` 得到改善。

#### 切割复杂语句到多个简单语句（主要优化内存占用）<br> & 使用 `merge` 代替 `update`

**例子 🌰4**

| 输入表     |      数据量 |
| ---------- | ----------: |
| 顾客信息表 | 145,586,869 |
| 业务表     |  48,094,285 |

| 中间表   |     数据量 |
| -------- | ---------: |
| 中间表 1 | 48,067,928 |
| 中间表 2 |     11,051 |

输出数据量为 11,051，并且更新顾客信息表对应对数据。

| &nbsp;             | 运行时间 |
| ------------------ | -------- |
| 修改前             | 报错     |
| 修改后（游标）     | 1:17:31  |
| 修改后（不用游标） | 0:09:51  |

这个例子中，修改前由于数据量原因运行了四五个小时之后报错了。类似于下面这个报错信息。这个情况是由于 UNDO 空间被写爆了。

> ORA-01555: snapshot too old: rollback segment number 1 with name "\_SYSSMU1\$" too small

优化的方法是分割复杂 SQL 到多个语句，中间加入中间表来衔接，并且使用 `merge` 代替 `update` 减少内存占有时间。

最终方案修改了两个版本，一个是使用游标进行分批处理，每次 1000 数据 `commit` 一次，减少内存负担，但是这个版本耗时太长了，plsql 引擎和 SQL 引擎来回切换颇为费时。

另外一版本直接使用 `insert select` 插入数据，虽然内存使用率变高，但速度提升显著，且十分钟的内存占有时间在接受范围内。最终使用了这个版本。

#### 修改业务逻辑

修改业务逻辑的场景一般没有普适性，需要结合数据和业务逻辑单独分析。

**例子 🌰5**

| 输入表     |      数据量 |
| ---------- | ----------: |
| 顾客表     | 138,066,840 |
| 顾客信息表 | 145,586,869 |

输出数据量为：13,764。并且途中做了几次不同程度的更新。

| &nbsp; | 运行时间 |
| ------ | -------- |
| 修改前 | 5:05:53  |
| 修改后 | 0:01:00  |

这个例子中就是上面所提到的修改业务逻辑的案例。

在正常逻辑下，我们抽出一批数据插入到目标表中，将一个 Flag 更新为初始值 `00`，然后根据各种表关联的结果再将其中一部分数据更新为 `99`。

但是实际数据测试中发现需要更新为 `99` 的数据量太大了，同时 `update` 本身又是一个昂贵的操作，所以整段 SQL 的执行效率奇慢无比。

分析数据和业务逻辑之后，我们反向操作，将 Flag 初始化更新为 `99`，然后将**不符合条件的数据**更新为 `00`，以此来减少了大量更新操作，大幅度提高了执行效率。

#### 一些其他的解决方案

上面是有具体案例的实践，下面还有一些试验过有效果的方案，但是由于一些原因没有运用到这次遇到的问题上去。都是一些有价值的方法，所以在这里也记录一下。

**使用 partition 表分区提高效率**

创建表分区虽然没有用来解决这次遇到的问题，但是实际上当初设计表的时候已经是做过分区的了。

表分区具体的作用是根据一定条件将一张大表分为几个不同的 `partition`，对外部来说这张表还是一个整体，但是优化器会自动判断，在符合条件的情况下访问对应的分区来提高效率。当然也可以在 SQL 语句中表名后面添加 `partition(partition_name)` 小句来指定访问哪个分区。

```sql
select count(1) from target_table partition(table_part01);
```

举一个具体的例子来说，假设顾客表中有两种类型的顾客，`企业` 顾客和 `个人` 顾客，这张表有 1 亿数据，如果我们按照顾客类型来给表进行分区，并且刚好每个分区有 5 千万数据的话，这时如果我们查询某个 `企业` 顾客的数据，查找范围就会从原本的 1 亿数据直接降到 5 千万数据，效率可谓直接提升一倍。

这个例子的 DDL 可能是下面这样的：

```sql
create table customer (
  id number,
  name varchar2(256),
  customer_type varchar2(20),
  -- other fields
) partition by list (customer_type) (
  partition customer_enterprise values ('enterprise'),
  partition customer_individual values ('individual')
);
```

> ⚠️ 表分区仅在企业版本可用。

官方文档建议的应该考虑创建表分区的场景：

- 表占空间大于 2G 以上；
- 表包含历史数据，典型的例子是按照月份整理的数据，仅更新当前月份的数据，而其他 11 个月的数据都是只读的；
- 表的内容必须分布到不同存储设备上。

参考资料：

- [2 Partitioning Concepts](https://docs.oracle.com/en/database/oracle/oracle-database/12.2/vldbg/partition-concepts.html#GUID-E849DE8A-547D-4A2E-9324-706CAF574754)

**减少 log 提高写入效率**

Oracle 中在执行 DML 语句时会产生 `UNDO` 和 `REDO` 日志来保护数据的完整性和安全性。`UNDO` 日志用来做事务的回滚，`REDO` 日志用来恢复事务。但是在大数据的情况下，大量的日志会很大程度地拉低执行效率。这时可以使用 `/*+ APPEND */` 来减少甚至不做 `UNDO` 日志，配合 `nologging` 来减少 `REDO` 日志，以此来提高执行效率。

使用 `/*+ APPEND */` 的 SQL 语句看起来是这样的。

```sql
insert /*+ APPEND */ into target_table (
  target_values,
  -- ...
);
```

使用 `nologging` 有两种方式，一种是从表定义上设定。

```sql
alter table target_table nologging;
```

或者写在 DML 语句中。

```sql
insert /*+ APPEND */ into target_table nologging (
  target_values,
  -- ...
);
```

**分割复杂 SQL 为多个简单 SQL，用中间表减少内存负担**

我们遇到的一些问题中，有一些比较复杂的 SQL 用 `with as` 做了一堆子查询，然后再将这些结果 `union` 到一起成为一个临时表，然后拿这个临时表继续往后面 `with as` 出更多的临时表...以此类推的情况，单独拿出其中每一个 `as` 小句出来执行效率都是可以接受的，但是由于 `union` 等操作花费时间比较长，占用资源无法释放等于提升了 cost。

对于这种情况，我们将复杂 SQL 分割成几条不那么复杂的 SQL，比如上面说的 `union` 的问题，实际代码上将 3 个子查询结果 `union` 到了一起，修改后在中间加一个中间表，三串子查询也变成三个独立的 SQL 语句，分别向中间表中做 3 次插入操作，以此来避免 `union`。

当然这样也带来了一些其他的问题，比如写中间表的操作会带来很多额外的 IO 成本，如果后续处理数据不是很多的话，还会让原本执行效率高的的操作因为多了一个写中间表的 IO 成本而拉低执行效率。

一个真实的例子是，我们有一条 SQL 文要更新 1 万条左右的数据，但是中间访问了几张 1 亿多条数据的大表所以造成执行效率很慢，我们通过添加了中间表的方式，将一个重复多次的子查询结果固定到一张中间表，数据一共四千八百万，IO 操作花了大半个小时。对于这次调整的结果来说确实提升了执行效率，但是更新 1 万条数据的情况属于特殊情况，平时可能只更新少量的数据，却由于多了这么一个 IO 操作导致每次无论最终处理数据量，这大半个小时的时间都是避免不了的了。

**用 `merge` 代替 `update` 做数据更新提高效率**

遇到性能问题的 SQL 有相当一部分是更新操作，甚至有一个任务要更新 700 万数据的 1 个 Flag，因为写满 `UNDO` 空间死活跑不过去的。可想而知更新操作是非常消耗资源的。

Oracle 中有一个 `merge` 语句，原本是用来聚合 `insert` 和 `update` 操作的，但是在新版本中可以单独做更新或者插入了，从网上资料中我们了解到 `merge` 操作比单独的 `update` 操作执行效率更高，于是将有问题的几个 SQL 用 `merge` 改写试了试。

其结果是，一条更新 45 万数据的 `update` 需要跑 7 个小时的操作，使用 `merge` 只花了 50 分钟就完成了。目前这个方案正在研讨中，还没有最终决定，但是使用 `merge` 可以提升更新效率是毋庸置疑的。

```sql
merge into table_a a using table_b b
on (conditions)
when matched then
  update set a.fields = b.fields
when not matched then
  insert(field_names) values(field_values);
```

如果仅做更新操作，`WHEN NOT MATCHED THEN` 后面的语句可以不需要。

**压缩表，利用空闲 CPU 减少 IO 时间**

作为最后的手段，压缩表也是提升执行效率的大杀器。

压缩表的原理是，给相同数据创建字典表，每次有相同数据进来，数据库将不储存原始数据，而仅储存字典表的一个引用。好处是压缩了表空间，大幅减少了 IO 操作，不过缺点也很明显，会显著提高 CPU 使用率。

对我们项目来说，数据库使用独立服务器，CPU 经常是闲置的，所以压缩表没有太大负担。

```sql
alter table target_table compress;
```

### 重现和实践

这是实践部分。

// TODO reproduce
